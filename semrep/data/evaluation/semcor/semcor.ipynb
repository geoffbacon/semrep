{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemCor\n",
    "\n",
    "SemCor is a WordNet annotated subset of the Brown corpus. WordNet has coarse features for nouns and verbs, called \"supersenses\". These are things like \"NOUN.BODY\", \"VERB.MOTION\". Supersenses are coarse semantic features, given by linguists. Once downloaded, NLTK provides easy access to the SemCor corpus, in particular as a stream of tagged chunks. Chunks are coarse constituents.\n",
    "\n",
    "## Features\n",
    "\n",
    "In this section, I derive feature representations for words from SemCor. This follows Tsvetkov et al. (2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus.reader.wordnet import Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_chunks = semcor.tagged_chunks(tag='both')\n",
    "tagged_chunks = list(tagged_chunks) # takes ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the tagged chunks into a `pandas dataframe`. Each row holds a chunk, and I will add columns as I go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pd.DataFrame({'chunk': tagged_chunks})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions correspond to the columns I want to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(tc):\n",
    "    \"\"\"Return the words of a tagged chunk.\"\"\"\n",
    "    words = [w.lower() for w in tc.leaves()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def extract_pos(tc):\n",
    "    \"\"\"\n",
    "    Return the POS of a tagged chunk.\n",
    "    \n",
    "    This isn't the cleanest way, but it works for now.\n",
    "    \"\"\"\n",
    "    return tc.pos()[0][1]\n",
    "\n",
    "def extract_supersense(tc):\n",
    "    \"\"\"\n",
    "    Return the supersense of a tagged chunk, otherwise None.\n",
    "    \n",
    "    Only nouns and verbs have supersenses.\n",
    "    \"\"\"\n",
    "    label = tc.label()\n",
    "    if isinstance(label, Lemma):\n",
    "        return label.synset().lexname()\n",
    "    return None\n",
    "\n",
    "def extract_rough_pos(supersense):\n",
    "    \"\"\"Return coarser POS from supersense information.\"\"\"\n",
    "    if supersense:\n",
    "        return supersense.split('.')[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc['words'] = sc['chunk'].apply(extract_words)\n",
    "sc['pos'] = sc['chunk'].apply(extract_pos)\n",
    "sc['supersense'] = sc['chunk'].apply(extract_supersense)\n",
    "sc['rough_pos'] = sc['supersense'].apply(extract_rough_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only nouns and verbs have supersenses in WordNet (adjectives do, but they seem to be ignored in the original paper). We only care about words that have supersenses now. Also, there's a lot of junk words in SemCor. First we replace all digits with '0'. Then we'll drop words that are i) not alphabetic or '0' or ii) multiple words. I can't tell how this is done in the original paper, so I may get slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = sc[sc['rough_pos'].isin(['noun', 'verb'])]\n",
    "sc.loc[sc['words'].str.isdigit(), 'words'] = '0'\n",
    "sc = sc[(sc['words'].str.isalpha()) | (sc['words'].str.match('0'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linguistic features are in fact a distribution over the 41 supersenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = sc.groupby('words')\n",
    "features = grouped['supersense'].value_counts(normalize=True).unstack()\n",
    "features['count_in_semcor'] = grouped['supersense'].count()\n",
    "features.reset_index(inplace=True) # make \"words\" a column not the index\n",
    "features.fillna(0, inplace=True)\n",
    "features.columns.name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restrict to words with a certain frequency in the corpus. The original paper has 4199 words when thresholding at a frequency of 5, I get 4847. I believe this discrepancy comes from how words are preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 4847\n"
     ]
    }
   ],
   "source": [
    "threshold = 5\n",
    "subset = features[features['count_in_semcor'] > threshold]\n",
    "print('Number of words:', len(subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save my features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset.to_csv('my_semcor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with reference implementation\n",
    "\n",
    "I want to know how different my linguistic features are from the original paper's, available [here](https://github.com/ytsvetko/qvec/blob/master/oracles/semcor_noun_verb.supersenses.en). Read in the original paper's. Note that their columns start with \"semcor.\", which will be helpful when I merge the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, dicts = [], []\n",
    "with open('tsvetkov.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word, _, d = line.partition('\\t')\n",
    "        words.append(word)\n",
    "        d = eval(d.strip())\n",
    "        dicts.append(d)\n",
    "\n",
    "tsvetkov = pd.DataFrame(dicts)\n",
    "tsvetkov['words'] = words\n",
    "tsvetkov.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Tsvetkov et al.'s features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvetkov.to_csv('tsvetkov_semcor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two sets. This dataframe has 84 columns: one for the word, one for my count in SemCor, 41 for my features, 41 for their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>noun.Tops</th>\n",
       "      <th>noun.act</th>\n",
       "      <th>noun.animal</th>\n",
       "      <th>noun.artifact</th>\n",
       "      <th>noun.attribute</th>\n",
       "      <th>noun.body</th>\n",
       "      <th>noun.cognition</th>\n",
       "      <th>noun.communication</th>\n",
       "      <th>noun.event</th>\n",
       "      <th>...</th>\n",
       "      <th>semcor.verb.consumption</th>\n",
       "      <th>semcor.verb.contact</th>\n",
       "      <th>semcor.verb.creation</th>\n",
       "      <th>semcor.verb.emotion</th>\n",
       "      <th>semcor.verb.motion</th>\n",
       "      <th>semcor.verb.perception</th>\n",
       "      <th>semcor.verb.possession</th>\n",
       "      <th>semcor.verb.social</th>\n",
       "      <th>semcor.verb.stative</th>\n",
       "      <th>semcor.verb.weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ability</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abolish</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  noun.Tops  noun.act  noun.animal  noun.artifact  noun.attribute  \\\n",
       "0        0        0.0       0.0          0.0            0.0        0.000000   \n",
       "1        a        0.0       0.0          0.0            0.0        0.000000   \n",
       "2  abandon        0.0       0.0          0.0            0.0        0.083333   \n",
       "3  ability        0.0       0.0          0.0            0.0        0.727273   \n",
       "4  abolish        0.0       0.0          0.0            0.0        0.000000   \n",
       "\n",
       "   noun.body  noun.cognition  noun.communication  noun.event  \\\n",
       "0        0.0        0.000000                 0.0         0.0   \n",
       "1        0.0        0.000000                 0.0         0.0   \n",
       "2        0.0        0.000000                 0.0         0.0   \n",
       "3        0.0        0.272727                 0.0         0.0   \n",
       "4        0.0        0.000000                 0.0         0.0   \n",
       "\n",
       "          ...           semcor.verb.consumption  semcor.verb.contact  \\\n",
       "0         ...                               0.0                  0.0   \n",
       "1         ...                               0.0                  0.0   \n",
       "2         ...                               0.0                  0.0   \n",
       "3         ...                               0.0                  0.0   \n",
       "4         ...                               0.0                  0.0   \n",
       "\n",
       "   semcor.verb.creation  semcor.verb.emotion  semcor.verb.motion  \\\n",
       "0                   0.0                  0.0                 0.0   \n",
       "1                   0.0                  0.0                 0.0   \n",
       "2                   0.0                  0.0                 0.0   \n",
       "3                   0.0                  0.0                 0.0   \n",
       "4                   0.0                  0.0                 0.0   \n",
       "\n",
       "   semcor.verb.perception  semcor.verb.possession  semcor.verb.social  \\\n",
       "0                     0.0                0.000000                 0.0   \n",
       "1                     0.0                0.000000                 0.0   \n",
       "2                     0.0                0.571429                 0.0   \n",
       "3                     0.0                0.000000                 0.0   \n",
       "4                     0.0                0.000000                 1.0   \n",
       "\n",
       "   semcor.verb.stative  semcor.verb.weather  \n",
       "0                  0.0                  0.0  \n",
       "1                  0.0                  0.0  \n",
       "2                  0.0                  0.0  \n",
       "3                  0.0                  0.0  \n",
       "4                  0.0                  0.0  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.merge(features, tsvetkov, on=['words'], how='inner')\n",
    "comparison.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I have two discrete distributions, and I want to know how different/similar they are. I need to ask someone how to do this properly, but I have a few naive ideas.\n",
    "\n",
    "#### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the columns so that features in the two lists are in the same position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "their_columns = sorted([c for c in comparison.columns if c.startswith('semcor')])\n",
    "my_columns = [c.partition('.')[2] for c in their_columns]\n",
    "\n",
    "def KL(row):\n",
    "    \"\"\"\n",
    "    Helper function for apply KL to a dataframe.\n",
    "    \n",
    "    I get errors unless I explicitly convert to float, even though they appear to be floats anyway\n",
    "    \"\"\"\n",
    "    theirs = row[their_columns].values.astype(float)\n",
    "    mine = row[my_columns].values.astype(float)\n",
    "    return stats.entropy(theirs, mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comparison['KL'] = comparison.apply(KL, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inf` is not good, meaning that the two distributions are very different. Nearly half of the words have very different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46724565756823822"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isinf(comparison['KL']).sum() / len(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, most words have similar distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEFCAYAAABAVTQtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlUuzB3kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130083208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(comparison[np.isfinite(comparison['KL'])]['KL']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
